{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from random import random\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "device = torch.device(0 if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from train/test data files and return the tuple as (label, original_sent, candsent, trendid)\n",
    "def readInData(filename):\n",
    "\n",
    "    data = []\n",
    "    trends = set([])\n",
    "    \n",
    "    (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = (None, None, None, None, None, None, None)\n",
    "    \n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        #read in training or dev data with labels\n",
    "        if len(line.split('\\t')) == 7:\n",
    "            (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = line.split('\\t')\n",
    "        #read in test data without labels\n",
    "        elif len(line.split('\\t')) == 6:\n",
    "            (trendid, trendname, origsent, candsent, origsenttag, candsenttag) = line.split('\\t')\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        #if origsent == candsent:\n",
    "        #    continue\n",
    "        \n",
    "        trends.add(trendid)\n",
    "        \n",
    "        if judge == None:\n",
    "            data.append((judge, origsent, candsent, trendid))\n",
    "            continue\n",
    "\n",
    "        # ignoring the training/test data that has middle label \n",
    "        if judge[0] == '(':  # labelled by Amazon Mechanical Turk in format like \"(2,3)\"\n",
    "            nYes = eval(judge)[0]            \n",
    "            if nYes >= 3:\n",
    "                amt_label = True\n",
    "                data.append((amt_label, origsent, candsent, trendid))\n",
    "            elif nYes <= 1:\n",
    "                amt_label = False\n",
    "                data.append((amt_label, origsent, candsent, trendid))   \n",
    "        elif judge[0].isdigit():   # labelled by expert in format like \"2\"\n",
    "            nYes = int(judge[0])\n",
    "            if nYes >= 4:\n",
    "                expert_label = True\n",
    "                data.append((expert_label, origsent, candsent, trendid))\n",
    "            elif nYes <= 2:\n",
    "                expert_label = False\n",
    "                data.append((expert_label, origsent, candsent, trendid))     \n",
    "            else:\n",
    "                expert_label = None\n",
    "                data.append((expert_label, origsent, candsent, trendid))        \n",
    "                \n",
    "    return data, trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(embedding_path, d_model):\n",
    "    d = {}\n",
    "    embedding_list = []\n",
    "    with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        idx = 1\n",
    "        while line:\n",
    "            try:\n",
    "                k = line.split()\n",
    "                a = [float(w) for w in k[1:]]\n",
    "                if (len(a)==d_model):\n",
    "                    d[k[0].lower()] = idx\n",
    "                    idx += 1\n",
    "                    embedding_list.append(a)\n",
    "            except:\n",
    "                pass\n",
    "            line = f.readline()\n",
    "    tmp = []\n",
    "    for i in range(d_model):\n",
    "        tmp.append(0)\n",
    "    embedding_list = [tmp] + embedding_list\n",
    "    embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_list), padding_idx=0)\n",
    "\n",
    "    print('Reading embedding finished.')\n",
    "        \n",
    "    return d, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(x, max_len=10000):\n",
    "#     max_len = 0\n",
    "#     for xx in x:\n",
    "#         if max_len < len(xx):\n",
    "#             max_len = len(xx)\n",
    "    for i in range(len(x)):\n",
    "        xx = x[i]\n",
    "        kk = len(xx)\n",
    "        x[i] = xx + ([0] * (max_len - kk)) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(d, sentence):\n",
    "    s=sentence.strip().split()\n",
    "    for i in range(len(s)):\n",
    "        s[i]=s[i].lower()\n",
    "        if s[i] in d.keys():\n",
    "            s[i]=d[s[i]]\n",
    "        else:\n",
    "            s[i]=0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(embedding_path, input_path, testing=False, d_model=200):\n",
    "    d, embedding = generate_dict(embedding_path, d_model)\n",
    "    x0 = []\n",
    "    x1 = []\n",
    "    y = []\n",
    "    max_len = 0\n",
    "    trends, _ = readInData(input_path)\n",
    "\n",
    "    for trend in trends:\n",
    "        if testing:\n",
    "            x0.append(get_index(d, trend[1]))\n",
    "            x1.append(get_index(d, trend[2]))\n",
    "            y.append(-1)\n",
    "        else:\n",
    "            if trend[0] == True:\n",
    "                x0.append(get_index(d, trend[1]))\n",
    "                x1.append(get_index(d, trend[2]))\n",
    "                y.append(0)\n",
    "            elif trend[0] == False:\n",
    "                x0.append(get_index(d, trend[1]))\n",
    "                x1.append(get_index(d, trend[2]))\n",
    "                y.append(1)\n",
    "\n",
    "    max_len = 0\n",
    "    for xx in x0 + x1:\n",
    "        if max_len < len(xx):\n",
    "            max_len = len(xx)\n",
    "    print(\"max length is: \", max_len)\n",
    "    embedding=embedding.to(device)\n",
    "    x0 = embedding(torch.tensor(padding(x0, max_len=max_len)).to(device))    \n",
    "    x1 = embedding(torch.tensor(padding(x1, max_len=max_len)).to(device))    \n",
    "\n",
    "    return x0.cpu(), x1.cpu(), torch.tensor(y, dtype=torch.float), embedding.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = '../tmp/attention_model'\n",
    "\n",
    "# Data & embedding configerations\n",
    "d_model = 200\n",
    "PRE_TRAINED_EMBEDDING_PATH = '../embedding/glove.twitter.27B.'+str(d_model)+'d.txt'\n",
    "DATA_PATH = '../data/train.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding finished.\n",
      "max length is:  18\n"
     ]
    }
   ],
   "source": [
    "x0, x1, Y, emb = preprocessing(PRE_TRAINED_EMBEDDING_PATH, DATA_PATH, testing=False, d_model=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11530, 18, 200])\n",
      "torch.Size([11530, 18, 200])\n",
      "torch.Size([11530])\n",
      "tensor([[ 1.2212e-01,  3.3079e-01,  1.6658e-01, -3.7311e-01, -3.2807e-01,\n",
      "         -5.2256e-01, -6.7980e-01,  2.9447e-01, -5.5401e-01,  5.1494e-01,\n",
      "         -4.6707e-02, -3.5564e-01,  9.8064e-02, -3.6815e-02, -1.2640e-01,\n",
      "         -3.9342e-01,  6.0168e-01, -3.4685e-01, -9.8971e-02,  1.4753e-01,\n",
      "         -7.1833e-02,  3.2310e-01,  4.3638e-01,  1.7693e-01, -2.7088e-01,\n",
      "         -1.1009e+00, -6.7499e-02,  3.9490e-02, -7.7714e-02,  1.0484e-01,\n",
      "          4.9229e-01,  3.8817e-01,  4.7439e-02, -5.2111e-02, -2.9466e-01,\n",
      "          3.1889e-01, -3.6786e-01, -1.5086e-01, -2.7244e-02,  2.4142e-01,\n",
      "          1.8413e-01,  5.3505e-01,  2.9721e-01,  6.7245e-02, -4.7623e-01,\n",
      "          2.4425e-01,  6.5088e-01,  3.2616e-01, -3.6000e-03, -1.0486e-01,\n",
      "          1.5229e-01,  6.0477e-01,  9.4309e-02,  3.5175e-01, -1.8084e-01,\n",
      "         -5.4886e-01, -8.2122e-03,  4.8639e-02, -1.4380e-01, -2.2617e-02,\n",
      "          1.9567e-01,  1.4418e-01, -3.4762e-01,  3.9937e-01,  6.5462e-01,\n",
      "          6.4702e-01,  2.1196e-01, -1.0199e-01, -4.0930e-01, -1.1965e-01,\n",
      "          1.9102e-02,  2.9345e-02,  2.5759e-01, -5.6539e-01,  2.8578e-01,\n",
      "         -6.1040e-02,  6.3072e-01,  5.9010e-01, -1.7740e-01, -6.3735e-01,\n",
      "          1.7219e-03, -5.1482e-02, -3.3951e-01, -2.3685e-01,  9.3136e-02,\n",
      "         -5.4001e-01, -1.0749e-01, -7.9117e-01, -4.1936e-01, -2.1682e-02,\n",
      "         -4.1917e-02,  2.6912e-01, -4.6845e-01,  2.2145e-01,  5.0716e-01,\n",
      "         -5.5348e-01, -1.8316e-02,  2.4520e-01,  3.5051e-01,  5.6133e-01,\n",
      "          7.6032e-01,  3.6503e-02,  1.4507e-01,  8.4835e-02,  7.5665e-01,\n",
      "         -3.9414e-03, -3.9358e-03, -6.2125e-01,  1.5639e-01,  5.0692e-01,\n",
      "         -6.5716e-01, -2.1911e-01, -3.8659e-01, -6.2229e-01, -1.9903e-01,\n",
      "          1.8310e-01,  2.2747e-01,  4.4559e-01,  1.9482e-01,  3.0399e-01,\n",
      "          5.8553e-01, -1.1756e-01, -3.6682e-01,  6.6500e-01,  4.3660e-02,\n",
      "          1.2517e-01,  4.1509e-01,  7.9264e-01,  2.9527e-01,  1.0914e-01,\n",
      "         -8.1515e-01, -6.5666e-01, -9.1767e-02,  1.8982e-01,  6.1701e-01,\n",
      "         -1.5243e-01,  1.3749e-01,  8.0982e-01,  2.2204e-01,  8.5301e-01,\n",
      "         -4.3516e-01,  3.7323e-01,  2.1415e-01,  1.8165e-01, -3.4729e-01,\n",
      "         -2.3002e-01,  9.5101e-01,  1.2885e-01, -7.0027e-02, -5.4468e-01,\n",
      "          3.3034e-01,  4.2494e-01, -1.9956e+00,  9.5754e-01,  7.4615e-01,\n",
      "         -9.0898e-02, -2.5643e-01,  4.8979e-02, -5.0406e-01,  2.6520e-02,\n",
      "         -1.5657e-01,  3.7299e-05,  2.2958e-01, -2.3606e-01, -1.0096e-01,\n",
      "         -1.7958e-01,  2.6625e-01,  5.4262e-01, -1.2818e-01, -3.9489e-01,\n",
      "         -3.3762e-01, -3.6616e-01, -8.2881e-02, -2.5533e-01, -8.4290e-01,\n",
      "         -3.0698e-01, -3.5447e-01,  2.0592e-01,  1.0835e-01,  1.3728e-01,\n",
      "          7.0204e-01,  1.7707e-01, -6.7478e-01, -1.8323e-01,  2.0432e-01,\n",
      "         -1.5562e-01, -1.0133e-01,  2.8185e-02, -5.1151e-01,  3.6433e-01,\n",
      "         -5.3552e-01, -1.4817e-01, -1.7968e-01, -1.5661e-01, -9.0872e-01,\n",
      "          3.0488e-01,  7.6037e-01, -4.1900e-02, -4.6526e-01,  2.3928e-02],\n",
      "        [ 7.1369e-02,  4.1956e-01,  5.5916e-03, -7.2201e-01,  1.9411e-01,\n",
      "          6.0349e-02, -4.6664e-01,  3.5384e-01, -5.9667e-01, -9.0338e-02,\n",
      "         -2.9097e-01,  3.2906e-01, -6.2929e-01,  1.3275e-01,  6.4791e-01,\n",
      "         -5.4285e-01, -6.0498e-01,  6.4092e-02,  7.8253e-02,  2.4275e-01,\n",
      "          4.5839e-01,  1.3984e-01, -1.8445e-01, -8.0360e-01, -7.4585e-01,\n",
      "         -1.8162e+00,  2.5454e-01,  5.4500e-01, -1.4881e-01, -6.4649e-02,\n",
      "          2.9090e-01,  5.1717e-01,  1.3385e-01, -4.3527e-01,  6.7974e-01,\n",
      "         -2.8983e-01, -4.0296e-01,  5.3341e-01,  3.6039e-02, -6.0069e-02,\n",
      "          2.6143e-01, -2.6273e-02,  6.4493e-01,  3.6396e-01, -1.0338e+00,\n",
      "          6.2254e-01,  4.9210e-03,  4.9586e-01, -2.4630e-01,  2.4622e-01,\n",
      "          2.9258e-01, -4.2850e-01,  6.0998e-01, -3.7189e-01, -2.0580e-01,\n",
      "         -4.2797e-01,  8.1299e-01, -7.9390e-01,  1.7219e-01, -6.5681e-02,\n",
      "          1.7180e-01, -9.9643e-02, -3.7750e-01, -7.2502e-02,  9.8512e-01,\n",
      "          3.1904e-01, -1.1292e-01, -3.0985e-01, -3.3602e-01, -5.6422e-02,\n",
      "          6.4468e-02, -4.5131e-02,  4.0795e-01,  2.9209e-02, -6.7427e-01,\n",
      "         -7.8312e-01,  4.8956e-01, -1.5703e-02, -2.4746e-01,  2.0588e-01,\n",
      "          2.5467e-01,  1.6869e-01,  4.2089e-02, -3.8892e-01, -2.9968e-01,\n",
      "          2.0284e-01, -4.9267e-01, -2.2968e-01, -3.1092e-01, -2.2982e-01,\n",
      "         -7.6024e-02, -2.7250e-01,  6.4072e-01, -2.3135e-01,  6.2814e-01,\n",
      "          8.7939e-01,  5.5152e-03, -1.9343e-01, -1.1239e-02, -2.9892e-01,\n",
      "          2.3969e-02,  1.1112e-01,  1.5471e-01,  3.4318e-01,  1.0841e+00,\n",
      "         -2.0269e-02,  3.9988e-02, -1.7468e-01, -2.2008e-01,  7.9166e-01,\n",
      "          9.1940e-02,  6.2424e-01,  7.9152e-02, -1.9058e-01, -2.1426e-01,\n",
      "          7.3296e-01, -4.4365e-01, -3.4749e-01,  2.2722e-01,  1.9064e-01,\n",
      "          9.1738e-01, -3.7414e-01,  2.9424e-01, -5.5521e-02, -2.1909e-01,\n",
      "          9.6065e-02,  3.8554e-01,  7.2331e-01,  3.1590e-02,  1.4686e-01,\n",
      "         -6.0974e-02,  2.9265e-01, -1.4372e-01, -6.7377e-02, -1.1385e-01,\n",
      "         -6.6238e-01,  4.9174e-01, -7.8025e-01,  1.9007e-01,  2.0116e-01,\n",
      "          1.0155e-01,  2.4557e-01,  8.3774e-02,  4.7483e-03,  1.0285e-01,\n",
      "         -1.4541e-01,  3.5038e-01, -1.0365e-01, -3.7940e-01, -3.2937e-01,\n",
      "          1.9356e-01,  1.8699e-01, -1.6182e+00,  4.1600e-01, -2.7802e-01,\n",
      "          4.3624e-02, -2.2472e-01,  2.8197e-01, -2.8016e-01, -5.0681e-02,\n",
      "         -5.5765e-01,  1.4199e-01, -1.0777e-01,  3.0869e-01, -1.8859e-01,\n",
      "         -3.3233e-01, -4.9190e-02, -1.8682e-01,  2.1488e-01, -7.7720e-01,\n",
      "         -2.1719e-01,  2.0840e-01,  1.0304e+00, -9.7718e-03, -2.0452e-01,\n",
      "          3.2720e-01,  3.7569e-01, -3.5491e-01, -5.7958e-01, -9.6210e-02,\n",
      "          3.3131e-01, -2.6256e-01,  3.8028e-01, -4.2414e-01,  5.5567e-01,\n",
      "          3.7492e-01, -3.5828e-01,  2.0673e-01,  2.4869e-02, -1.9814e-01,\n",
      "         -1.9999e-01,  3.4344e-01,  3.6661e-01,  2.0107e-02,  5.6687e-01,\n",
      "         -4.9489e-01,  8.9241e-03, -3.9948e-02,  5.6716e-01,  2.2437e-01],\n",
      "        [ 4.9350e-01,  3.5698e-01,  6.6068e-01, -3.2975e-02,  2.4989e-01,\n",
      "          2.5936e-01, -2.7169e-02,  6.8403e-02, -2.9063e-01, -4.5705e-01,\n",
      "         -7.7940e-02,  3.2516e-01, -1.4852e+00, -6.7472e-02, -1.7033e-01,\n",
      "         -9.2964e-03,  3.4628e-01, -1.1575e-02,  3.7965e-02,  4.5617e-01,\n",
      "          8.0486e-02,  1.5310e-01, -1.5309e-01, -1.8810e-01, -1.8195e-01,\n",
      "          8.7245e-01,  3.9792e-01,  4.0995e-01,  4.4982e-01, -1.9651e-03,\n",
      "         -4.1124e-02, -4.7867e-02, -2.4045e-01, -8.6883e-02,  1.4180e-02,\n",
      "         -2.3760e-01,  2.5183e-01,  2.8551e-01,  4.4504e-01, -4.9644e-01,\n",
      "         -1.2709e-01, -1.7476e-01,  8.2196e-02,  4.5409e-02,  5.1720e-01,\n",
      "          3.4546e-02, -8.5822e-02, -3.4924e-01,  5.2215e-01, -3.9510e-01,\n",
      "          6.4123e-02, -4.2020e-01, -1.5943e-01,  1.8283e-01, -5.7885e-02,\n",
      "         -1.9182e-02, -4.4547e-01,  3.1542e-01, -1.6096e-01, -9.2139e-02,\n",
      "         -2.4960e-01, -1.3897e-03, -4.2643e-01, -1.7932e-01,  8.1654e-02,\n",
      "          1.8324e-01, -3.2065e-01, -1.2009e-04, -1.3095e-01, -2.9812e-01,\n",
      "         -2.5144e-03, -1.1318e-01, -4.6378e-01, -1.9960e-01,  8.5100e-01,\n",
      "         -6.8009e-02,  1.2870e-01, -6.7256e-01, -1.2225e-02,  1.4201e-01,\n",
      "          9.2478e-01,  8.5156e-02,  2.6888e-01,  3.0390e-01, -1.4030e-01,\n",
      "          1.8599e-01, -2.5526e-01,  1.8381e-01, -3.5222e-01, -7.8399e-02,\n",
      "         -2.6944e-01, -1.8083e-01, -2.7305e-02, -3.9332e-01,  2.4542e-01,\n",
      "         -1.1199e-01,  1.2093e-01, -4.6106e-01,  3.7773e-03,  2.3799e-01,\n",
      "         -1.9402e-01,  1.5473e-01, -9.9502e-04,  9.2256e-02,  3.9850e-01,\n",
      "         -3.8062e-01, -3.6498e-01,  4.9292e-03, -7.3963e-02,  1.4034e-01,\n",
      "         -2.3085e-01,  1.3001e-01,  3.0408e-01,  1.0590e-01,  7.8896e-02,\n",
      "          8.6252e-02,  5.8663e-02,  1.7926e-01, -1.5100e-01,  9.5240e-01,\n",
      "          3.7305e-01,  2.2537e-02,  3.2819e-02, -4.9961e-01,  1.6748e-01,\n",
      "          3.0390e-01,  3.3058e-01, -4.7308e-01,  5.6388e-01,  5.7122e-01,\n",
      "          5.5669e-02, -2.9073e-01, -1.7466e-01, -1.3091e-01,  1.5409e-01,\n",
      "         -2.1460e-01,  3.1676e-01,  2.0931e-01,  1.1919e-02, -3.4405e-01,\n",
      "          1.1990e-01,  3.9104e-01, -4.1181e-01,  2.0840e-01,  1.9732e-01,\n",
      "          4.0073e-01,  4.1617e-01,  7.2766e-02, -3.3984e-01,  2.0382e-01,\n",
      "          8.6184e-02,  2.8313e-01, -6.7626e+00, -5.6832e-01,  1.4953e-01,\n",
      "         -2.4673e-01,  6.0453e-01,  1.2149e-01, -5.2715e-01,  1.3591e-01,\n",
      "          1.5676e-01,  5.8968e-02, -3.2974e-01,  2.1303e-02,  1.2750e-03,\n",
      "          2.4996e-01, -2.4638e-02,  2.5518e-01,  3.8581e-01, -2.4223e-01,\n",
      "         -1.8271e-01,  2.9610e-01,  2.1855e-01,  1.9233e-01,  1.5229e-01,\n",
      "         -1.3649e-02, -5.0622e-02, -9.6446e-02,  1.5646e-02,  5.1167e-01,\n",
      "          1.4723e-01, -8.1492e-02, -3.1069e-01, -1.1167e-01, -1.3433e-01,\n",
      "          3.6670e-01, -2.9659e-01, -4.9952e-01, -6.0474e-02,  4.6954e-03,\n",
      "         -1.6617e-01, -1.7018e-01,  1.1233e-01,  1.6040e-01, -1.7142e-01,\n",
      "         -1.5621e-01, -1.1366e-01,  1.7706e-01, -5.3695e-01, -2.9699e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 3.0789e-01,  4.3085e-01,  3.8492e-01,  1.1262e-02,  3.1826e-01,\n",
      "          1.3479e-01,  2.4282e-01,  2.6534e-01, -5.2146e-02,  4.8128e-01,\n",
      "         -2.6013e-01, -1.0986e-01, -4.6310e-01, -2.9378e-01,  7.4562e-01,\n",
      "         -7.6866e-01, -7.4317e-02, -4.9875e-01, -3.3209e-01, -1.6178e-01,\n",
      "         -1.8491e-01,  4.2489e-01,  5.0163e-01, -5.4667e-01, -3.2127e-01,\n",
      "          7.5606e-01, -5.8326e-01,  4.0702e-01, -3.1600e-01,  8.3631e-02,\n",
      "          9.2858e-01,  2.2165e-01,  3.5796e-01, -6.2185e-02, -4.5383e-02,\n",
      "         -8.4724e-02,  3.0864e-01, -6.9766e-01, -2.3354e-01, -7.1734e-01,\n",
      "          1.1825e+00,  4.6360e-02, -6.5609e-01,  4.2081e-01, -2.2181e-02,\n",
      "          3.6320e-01,  3.0796e-01, -1.0243e-01,  2.9246e-01,  4.0648e-01,\n",
      "         -1.3771e-01,  4.4950e-01,  9.3130e-02,  5.5940e-01, -1.0232e+00,\n",
      "          8.9851e-02,  3.0530e-01, -1.1281e-02, -3.6278e-01,  2.5152e-01,\n",
      "          1.4041e-01, -6.1687e-02, -7.8482e-01,  3.9296e-01,  2.7274e-01,\n",
      "          6.4925e-01,  3.9312e-01, -1.6951e-01,  2.0878e-01,  1.6812e-01,\n",
      "          4.3147e-01,  3.5613e-01, -8.2755e-01, -4.8121e-01, -1.0113e+00,\n",
      "         -7.7007e-02,  7.6324e-01,  3.7122e-01, -7.3703e-01, -7.0033e-01,\n",
      "          7.2884e-01,  1.2187e-01, -1.4441e-01,  9.8331e-01,  3.4754e-01,\n",
      "         -1.3984e-01, -1.0627e-02,  5.5667e-02,  2.3652e-01, -1.9986e-01,\n",
      "         -6.5990e-02,  4.0545e-01, -7.0871e-01, -1.8794e-01, -3.0555e-02,\n",
      "          2.5414e-01, -3.8038e-01,  4.6401e-01, -6.7715e-03, -2.8329e-02,\n",
      "          2.0875e-01, -6.6878e-02,  1.0347e+00, -7.6221e-01,  8.5110e-01,\n",
      "         -3.7192e-01,  1.0649e-01,  1.9053e-02,  4.0171e-01, -6.7891e-01,\n",
      "          4.1458e-01,  1.2201e-01, -9.0355e-02,  5.9799e-01, -1.6751e-01,\n",
      "         -6.7336e-02,  3.1460e-01,  7.7561e-01,  1.4981e-01, -5.8855e-02,\n",
      "          3.9677e-01, -2.2259e-01, -7.6675e-01, -4.8620e-01, -1.0790e-01,\n",
      "         -5.1321e-01, -4.6080e-01,  7.2435e-01, -1.5796e-01,  4.2479e-01,\n",
      "         -1.1577e+00, -6.9358e-02,  4.0421e-01,  9.4385e-01, -1.5499e-01,\n",
      "          4.5141e-01,  3.9659e-01, -6.2645e-01, -6.3913e-01,  5.3033e-01,\n",
      "          2.0446e-01,  1.3816e-01, -1.4052e-02,  2.5430e-01,  6.9668e-02,\n",
      "         -5.7547e-01,  9.7113e-01, -4.4637e-01, -1.3389e-01, -8.6385e-01,\n",
      "         -1.6983e-02, -5.0783e-01, -2.5598e+00,  3.7735e-01,  4.5135e-02,\n",
      "         -1.7483e-02,  8.3408e-02,  2.1004e-01,  4.0047e-01, -4.8645e-01,\n",
      "         -1.3844e+00,  7.2605e-02,  6.2092e-02,  8.9095e-01,  6.6520e-01,\n",
      "          7.1008e-01,  4.2845e-01,  4.3104e-01,  6.5750e-01, -1.2739e+00,\n",
      "         -1.2167e-01,  7.1385e-02,  3.8428e-01,  7.9046e-01, -8.3978e-02,\n",
      "         -5.6752e-01, -4.9663e-01, -4.5684e-01,  2.3870e-01,  8.1209e-01,\n",
      "          6.2138e-01, -9.7909e-02,  3.2283e-02, -6.1945e-01,  2.8979e-01,\n",
      "          9.5202e-01,  1.0426e-01, -4.1289e-01, -7.9583e-01, -2.3082e-01,\n",
      "         -5.1061e-01,  3.7387e-01,  4.6171e-01,  2.8101e-02, -5.8805e-01,\n",
      "          1.6755e-01,  2.0728e-01, -4.1093e-01,  5.4623e-01, -3.0739e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(x0.size())\n",
    "print(x1.size())\n",
    "print(Y.size())\n",
    "print(x0[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(DATA_PATH+\"_\"+str(d_model)+\"d.pkl\", \"wb\")\n",
    "pickle.dump(x0, f)\n",
    "pickle.dump(x1, f)\n",
    "pickle.dump(Y, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
