{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from random import random\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "device = torch.device(0 if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from train/test data files and return the tuple as (label, original_sent, candsent, trendid)\n",
    "def readInData(filename):\n",
    "\n",
    "    data = []\n",
    "    trends = set([])\n",
    "    \n",
    "    (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = (None, None, None, None, None, None, None)\n",
    "    \n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        #read in training or dev data with labels\n",
    "        if len(line.split('\\t')) == 7:\n",
    "            (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = line.split('\\t')\n",
    "        #read in test data without labels\n",
    "        elif len(line.split('\\t')) == 6:\n",
    "            (trendid, trendname, origsent, candsent, origsenttag, candsenttag) = line.split('\\t')\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        #if origsent == candsent:\n",
    "        #    continue\n",
    "        \n",
    "        trends.add(trendid)\n",
    "        \n",
    "        if judge == None:\n",
    "            data.append((judge, origsent, candsent, trendid))\n",
    "            continue\n",
    "\n",
    "        # ignoring the training/test data that has middle label \n",
    "        if judge[0] == '(':  # labelled by Amazon Mechanical Turk in format like \"(2,3)\"\n",
    "            nYes = eval(judge)[0]            \n",
    "            if nYes >= 3:\n",
    "                amt_label = True\n",
    "                data.append((amt_label, origsent, candsent, trendid))\n",
    "            elif nYes <= 1:\n",
    "                amt_label = False\n",
    "                data.append((amt_label, origsent, candsent, trendid))   \n",
    "        elif judge[0].isdigit():   # labelled by expert in format like \"2\"\n",
    "            nYes = int(judge[0])\n",
    "            if nYes >= 4:\n",
    "                expert_label = True\n",
    "                data.append((expert_label, origsent, candsent, trendid))\n",
    "            elif nYes <= 2:\n",
    "                expert_label = False\n",
    "                data.append((expert_label, origsent, candsent, trendid))     \n",
    "            else:\n",
    "                expert_label = None\n",
    "                data.append((expert_label, origsent, candsent, trendid))        \n",
    "                \n",
    "    return data, trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(embedding_path, d_model):\n",
    "    d = {}\n",
    "    embedding_list = []\n",
    "    with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        idx = 1\n",
    "        while line:\n",
    "            try:\n",
    "                k = line.split()\n",
    "                a = [float(w) for w in k[1:]]\n",
    "                if (len(a)==d_model):\n",
    "                    d[k[0].lower()] = idx\n",
    "                    idx += 1\n",
    "                    embedding_list.append(a)\n",
    "            except:\n",
    "                pass\n",
    "            line = f.readline()\n",
    "    tmp = []\n",
    "    for i in range(d_model):\n",
    "        tmp.append(0)\n",
    "    embedding_list = [tmp] + embedding_list\n",
    "    embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_list), padding_idx=0)\n",
    "\n",
    "    print('Reading embedding finished.')\n",
    "        \n",
    "    return d, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(x, max_len=10000):\n",
    "#     max_len = 0\n",
    "#     for xx in x:\n",
    "#         if max_len < len(xx):\n",
    "#             max_len = len(xx)\n",
    "    for i in range(len(x)):\n",
    "        xx = x[i]\n",
    "        kk = len(xx)\n",
    "        x[i] = xx + ([0] * (max_len - kk)) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(d, sentence):\n",
    "    s=sentence.strip().split()\n",
    "    for i in range(len(s)):\n",
    "        s[i]=s[i].lower()\n",
    "        if s[i] in d.keys():\n",
    "            s[i]=d[s[i]]\n",
    "        else:\n",
    "            s[i]=0\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(embedding_path, input_path, testing=False, d_model=200, max_len=None):\n",
    "    d, embedding = generate_dict(embedding_path, d_model)\n",
    "    x0 = []\n",
    "    x1 = []\n",
    "    y = []\n",
    "    trends, _ = readInData(input_path)\n",
    "\n",
    "    for trend in trends:\n",
    "        if testing:\n",
    "            x0.append(get_index(d, trend[1]))\n",
    "            x1.append(get_index(d, trend[2]))\n",
    "            y.append(-1)\n",
    "        else:\n",
    "            if trend[0] == True:\n",
    "                x0.append(get_index(d, trend[1]))\n",
    "                x1.append(get_index(d, trend[2]))\n",
    "                y.append(0)\n",
    "            elif trend[0] == False:\n",
    "                x0.append(get_index(d, trend[1]))\n",
    "                x1.append(get_index(d, trend[2]))\n",
    "                y.append(1)\n",
    "    \n",
    "    if max_len==None:\n",
    "        max_len = 0\n",
    "        for xx in x0 + x1:\n",
    "            if max_len < len(xx):\n",
    "                max_len = len(xx)\n",
    "    print(\"max length is: \", max_len)\n",
    "    embedding=embedding.to(device)\n",
    "    x0 = embedding(torch.tensor(padding(x0, max_len=max_len)).to(device))    \n",
    "    x1 = embedding(torch.tensor(padding(x1, max_len=max_len)).to(device))    \n",
    "\n",
    "    return x0.cpu(), x1.cpu(), torch.tensor(y, dtype=torch.float), embedding.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = '../tmp/attention_model'\n",
    "\n",
    "# Data & embedding configerations\n",
    "d_model = 200\n",
    "PRE_TRAINED_EMBEDDING_PATH = '../embedding/glove.twitter.27B.'+str(d_model)+'d.txt'\n",
    "DATA_PATH = '../data/dev.data'\n",
    "OUTPUT_PATH = '../data/dev_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embedding finished.\n",
      "max length is:  18\n"
     ]
    }
   ],
   "source": [
    "x0, x1, Y, emb = preprocessing(PRE_TRAINED_EMBEDDING_PATH, DATA_PATH, testing=False, d_model=d_model, max_len=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4142, 18, 200])\n",
      "torch.Size([4142, 18, 200])\n",
      "torch.Size([4142])\n",
      "tensor([[ 1.4931e-01,  2.7889e-01,  8.9979e-02,  4.0882e-01, -2.1328e-01,\n",
      "          1.5406e-01, -2.5642e-02, -6.4515e-01, -7.1643e-01, -1.1794e-01,\n",
      "         -2.9600e-01, -4.3363e-01, -2.1885e-01,  3.2778e-02,  1.5606e-01,\n",
      "          2.2966e-02, -5.3795e-02,  3.3622e-01, -6.2113e-01,  1.0144e-01,\n",
      "          2.3716e-01, -5.1758e-02,  2.9100e-01, -4.3310e-01,  5.1603e-01,\n",
      "         -1.9666e+00,  2.0311e-01,  6.6447e-02,  1.5362e-01,  6.4771e-01,\n",
      "         -3.8559e-01,  4.7402e-03, -5.2268e-02, -1.0286e-01,  6.7909e-03,\n",
      "          5.1034e-01, -1.9149e-01, -1.0676e-01, -9.3639e-01,  2.3279e-01,\n",
      "         -6.8884e-01,  4.6741e-02,  1.0391e-01,  1.7044e-01,  5.3320e-01,\n",
      "         -1.6093e-01,  9.8364e-02,  3.6096e-01,  7.6576e-02,  4.0381e-01,\n",
      "         -2.1510e-02,  6.4061e-02, -3.2644e-01, -1.5550e-01, -1.4447e-02,\n",
      "          5.5337e-01,  2.5903e-01,  1.0481e-01,  3.1606e-01,  2.1116e-01,\n",
      "          3.0245e-01, -1.8877e-01, -5.3178e-01,  2.2984e-02,  7.7533e-01,\n",
      "         -1.7901e-01, -1.4461e-01,  7.9017e-02, -6.1084e-01, -3.2571e-01,\n",
      "         -2.4362e-01, -1.4472e-01,  1.7300e-01, -4.8226e-01,  7.9645e-02,\n",
      "         -1.7424e-01, -2.0305e-01,  8.7496e-02,  8.4781e-02,  1.9527e-01,\n",
      "          3.9270e-01,  1.6613e-01, -3.1525e-01, -7.9715e-02,  1.7212e-01,\n",
      "          4.5553e-01, -1.1114e-02, -1.1950e-01, -3.9213e-01,  4.9166e-01,\n",
      "          5.6016e-01,  2.4092e-01,  1.5597e-02,  3.3681e-01,  4.7240e-01,\n",
      "          1.7707e-01, -1.7481e-01, -2.6716e-01, -2.4148e-01, -6.5067e-01,\n",
      "          6.3830e-01,  3.7816e-01, -9.0254e-01, -4.2457e-01,  6.7957e-03,\n",
      "         -3.5392e-03, -3.8539e-01, -1.3497e-01, -2.6438e-01, -9.1551e-02,\n",
      "          4.5779e-01,  2.0661e-01,  3.1355e-01, -2.9931e-01, -2.0429e-01,\n",
      "          9.1683e-02, -1.4020e-01,  5.1077e-01,  1.0712e-01,  4.9198e-01,\n",
      "         -5.6160e-01,  5.4185e-02,  5.8032e-01, -1.6596e-01, -3.8643e-01,\n",
      "         -9.7051e-02,  5.8707e-01,  1.4964e-01,  2.7113e-01, -1.2814e-01,\n",
      "         -3.4048e-02,  9.2901e-02,  2.3331e-01, -2.7932e-01, -4.0010e-01,\n",
      "         -3.4193e-03, -3.9250e-01,  4.9183e-02, -4.3580e-01, -6.4382e-02,\n",
      "          3.7906e-01, -5.4632e-02, -6.7676e-01,  3.5143e-01,  6.5238e-01,\n",
      "          2.1358e-01,  3.5812e-01,  1.5221e-01, -2.5112e-01, -1.3765e-01,\n",
      "          9.1990e-02, -1.3461e-03, -6.0206e+00,  2.2602e-01, -2.5280e-01,\n",
      "         -1.9129e-01,  3.1382e-01,  4.4607e-01,  3.6630e-01,  7.5894e-02,\n",
      "          9.3947e-02, -1.5956e-01,  3.9989e-01,  6.3161e-01,  2.9423e-01,\n",
      "         -7.7584e-01, -2.3667e-01, -2.0197e-01,  1.7155e-02, -3.8523e-01,\n",
      "          6.6689e-01,  8.9704e-02, -3.1599e-01,  5.0257e-01,  5.1956e-01,\n",
      "          6.9124e-02,  1.7671e-01, -5.7517e-02,  7.5929e-01,  3.3151e-02,\n",
      "         -2.6773e-01,  1.4788e-01,  1.2935e-01,  2.0020e-01,  3.9306e-01,\n",
      "          1.8141e-01,  9.3880e-02, -3.7701e-01,  7.4141e-02,  2.5046e-01,\n",
      "          1.2651e-01,  4.6811e-01,  4.9654e-01, -1.8688e-01,  5.7106e-01,\n",
      "         -4.0908e-01, -4.0246e-02, -4.4821e-01,  3.1604e-01, -4.9365e-01],\n",
      "        [-4.7503e-01, -3.0273e-01,  4.8714e-01, -3.4793e-01,  1.5260e-01,\n",
      "          3.0337e-01,  8.9469e-01,  2.9891e-02, -1.2826e-01,  4.4509e-01,\n",
      "         -6.3352e-03, -3.7954e-01, -7.4339e-01, -6.9891e-02, -1.0674e+00,\n",
      "          3.0650e-01, -3.1633e-01, -1.1462e-01, -1.5391e-01, -2.0815e-01,\n",
      "          5.1554e-01,  1.4526e-01, -3.9960e-01, -2.5606e-01,  1.2889e-01,\n",
      "          1.2155e+00, -2.2213e-01, -1.0170e-01,  2.3039e-02, -2.3644e-01,\n",
      "          4.2401e-02,  2.3443e-01,  2.9783e-01,  5.1734e-02,  4.5916e-02,\n",
      "          6.8768e-02, -2.5251e-01, -1.7503e-01,  4.4819e-01,  3.2820e-01,\n",
      "          5.9687e-01,  5.0787e-01, -1.3184e-01,  3.9034e-02,  1.4703e-01,\n",
      "          4.8170e-01,  5.5703e-01,  4.3038e-01,  3.8466e-01,  3.6769e-01,\n",
      "          2.9980e-01, -3.9180e-01, -5.1086e-02, -6.0562e-01, -4.6074e-01,\n",
      "         -3.1795e-01, -4.6319e-01,  5.8673e-01,  2.9101e-01,  8.4474e-02,\n",
      "          5.1740e-01,  1.5022e-01,  4.1950e-01, -2.0262e-01, -6.2503e-02,\n",
      "         -5.9337e-01, -1.4632e-01,  3.3535e-01,  2.5083e-01,  7.3837e-02,\n",
      "         -2.6572e-01,  3.8996e-03, -5.5289e-01, -1.4402e-01,  2.3559e-01,\n",
      "         -1.6285e-01, -3.7756e-02, -5.7840e-01, -5.0175e-02, -5.0529e-01,\n",
      "          5.1635e-01, -4.4331e-01, -3.4867e-01,  7.2034e-02,  3.2792e-01,\n",
      "         -3.2363e-01,  1.5707e-01,  3.7529e-01,  2.7497e-01, -6.6156e-02,\n",
      "          2.8514e-01, -3.8786e-01,  3.2620e-01, -5.6480e-02,  3.2197e-01,\n",
      "          2.6162e-01,  1.9585e-01, -5.1911e-01,  2.3952e-01,  1.1327e-01,\n",
      "         -6.5479e-01, -1.3053e-02,  1.9080e-01, -2.6802e-02, -1.0578e-02,\n",
      "         -5.2591e-01,  1.3985e-01, -4.0095e-01, -7.0043e-02, -5.0483e-01,\n",
      "         -8.2242e-02,  2.3339e-02, -1.5862e-01,  3.4955e-01,  1.5561e-01,\n",
      "         -7.3182e-01, -2.3403e-01, -3.6284e-01,  4.3809e-01, -1.7894e-02,\n",
      "         -2.4537e-01, -1.0067e-01, -7.2684e-01,  3.9977e-01,  1.7327e-01,\n",
      "          6.6419e-01,  3.3052e-01, -4.5245e-01,  2.3188e-01,  1.0981e-01,\n",
      "          8.6084e-02,  1.0036e-02,  4.2484e-02, -7.3514e-01, -6.1841e-01,\n",
      "         -3.2588e-02, -3.1964e-02, -2.1592e-01,  2.2763e-01,  2.1098e-01,\n",
      "         -4.1950e-01, -2.2711e-01, -6.2230e-01, -3.7724e-02, -5.1999e-01,\n",
      "          5.9147e-01,  1.1428e-01, -2.0496e-02,  1.0604e-01,  2.4054e-01,\n",
      "         -3.2615e-01, -3.1021e-01, -4.3667e+00,  3.9268e-02, -1.4684e-01,\n",
      "          3.8366e-01, -2.9242e-01,  6.7046e-01,  2.5904e-01,  3.1059e-01,\n",
      "          3.8479e-01, -1.6322e-01,  1.3271e-02,  2.0379e-01, -3.7336e-01,\n",
      "         -6.3655e-02, -4.2308e-01, -9.7479e-02,  4.2201e-02, -6.9057e-01,\n",
      "         -6.1364e-01, -1.0671e-01, -6.0126e-01, -5.0754e-02,  3.6102e-02,\n",
      "          1.1275e-01,  1.1271e-01, -9.8255e-01,  2.0860e-01,  2.2742e-01,\n",
      "          1.6868e-01,  1.9554e-02,  4.6641e-01, -2.2040e-01,  5.2682e-01,\n",
      "          4.4892e-01,  7.3207e-02, -4.2608e-01,  6.5428e-01,  4.5329e-01,\n",
      "          4.5999e-01, -2.2457e-02,  3.8260e-02,  4.8137e-01, -2.9773e-02,\n",
      "          5.6729e-02,  6.3966e-01,  2.2473e-01, -2.1451e-02,  5.8194e-01],\n",
      "        [ 6.1774e-01,  2.1046e-01,  5.2698e-01,  2.0467e-01,  2.5700e-01,\n",
      "         -2.1732e-01,  6.1374e-01, -1.0005e-02,  3.0721e-01, -1.0223e-01,\n",
      "          2.7075e-01, -3.6910e-01, -1.4433e+00, -1.1167e-01, -1.1774e-02,\n",
      "         -6.1509e-01, -4.7574e-02,  2.9608e-01, -5.3990e-01, -6.6852e-01,\n",
      "         -1.0916e-01, -1.7703e-01, -4.5429e-01,  1.7385e-01,  2.9253e-01,\n",
      "          1.6115e-01,  1.9399e-01,  4.4625e-01,  8.7495e-01, -9.2465e-02,\n",
      "          1.3727e-02, -1.8580e-01, -3.3876e-01, -4.5230e-01, -4.3748e-01,\n",
      "          1.0453e-02,  4.0842e-01,  1.3613e-01, -7.8784e-01,  9.9471e-02,\n",
      "          8.8163e-02,  6.6798e-01,  3.4415e-01, -3.9527e-01,  2.3724e-01,\n",
      "          1.5362e-01, -4.9449e-01, -5.6937e-02, -1.4270e-01, -6.9449e-02,\n",
      "         -3.0868e-01, -7.6233e-02,  2.0988e-01,  7.8556e-02, -5.6155e-01,\n",
      "          1.8090e-01, -5.1773e-01,  4.6357e-01,  3.0987e-01, -1.9069e-02,\n",
      "          6.7242e-01,  3.9125e-01, -1.4125e-01,  1.1573e-02,  1.2692e-01,\n",
      "          8.8414e-02, -3.0354e-01,  3.2022e-01, -7.5043e-01,  6.4759e-01,\n",
      "          4.0745e-01, -3.2315e-01,  1.3080e-01,  1.1403e-01,  2.2156e-01,\n",
      "          1.3545e-01,  3.0976e-01, -3.5066e-01,  2.6475e-01, -1.8838e-01,\n",
      "          2.0806e-01,  2.4032e-01,  4.3735e-01,  3.4629e-01,  2.1611e-01,\n",
      "          3.5571e-01, -2.3089e-02,  3.4014e-01, -9.7253e-03, -3.8114e-01,\n",
      "          7.2125e-02,  3.1611e-02,  3.7476e-01,  1.8813e-01,  6.7863e-01,\n",
      "         -3.9233e-01, -7.5472e-02, -1.3099e-01, -3.8893e-01,  1.1880e-02,\n",
      "          4.1337e-01, -1.0998e-01,  3.0033e-01, -4.6344e-02,  2.2431e-01,\n",
      "         -6.3357e-01,  4.2110e-01,  6.1615e-02, -8.7939e-02, -1.0441e-01,\n",
      "         -1.4026e-01,  6.6755e-02,  4.7358e-01,  1.6719e-01,  1.0758e-01,\n",
      "         -2.2153e-01,  4.8319e-02,  4.2274e-01,  2.3313e-01,  3.6964e-01,\n",
      "          3.3876e-01,  4.1347e-01, -1.1237e-01,  1.5725e-01,  7.3065e-01,\n",
      "         -4.9247e-03,  2.7203e-01,  3.0763e-01,  4.7040e-01,  2.6032e-01,\n",
      "         -9.5454e-02, -7.6075e-01,  5.3049e-01,  9.0797e-02, -4.7060e-01,\n",
      "         -2.9850e-01,  2.8640e-01,  5.7581e-01,  5.8647e-02, -8.2910e-01,\n",
      "          1.5576e-01,  2.2733e-02, -4.5719e-02,  7.0358e-01,  3.4692e-01,\n",
      "          6.0275e-02,  1.8755e-01,  8.3105e-01,  6.5844e-02,  2.8287e-01,\n",
      "          5.2738e-01,  2.6252e-01, -6.6675e+00,  6.4621e-01,  7.5547e-03,\n",
      "          7.8997e-01, -5.2943e-01, -2.4546e-01,  6.3916e-01,  2.5545e-01,\n",
      "         -2.9200e-02,  1.9652e-01,  4.1215e-02,  1.5697e-01, -4.1102e-01,\n",
      "         -2.4685e-01,  1.9674e-01, -1.5722e-02, -6.4124e-02, -2.6484e-01,\n",
      "         -1.3809e-01,  1.7366e-01,  2.7951e-02, -3.7426e-02, -1.8403e-01,\n",
      "         -2.9065e-01,  7.0043e-02, -3.5120e-01,  2.6538e-01, -3.2504e-02,\n",
      "          2.4060e-01,  7.9199e-01, -1.2442e-01,  3.5116e-02, -4.1300e-01,\n",
      "          8.6633e-02,  4.0075e-01,  8.7781e-03,  2.9536e-01,  2.8309e-01,\n",
      "          1.5470e-01, -1.8914e-01,  2.0568e-01, -1.8938e-01,  3.3298e-02,\n",
      "          2.2822e-02,  3.3207e-01, -1.1269e-02, -5.2165e-01,  9.3783e-02],\n",
      "        [ 4.4608e-01,  6.3525e-01,  2.8992e-01,  1.9147e-01, -3.4412e-01,\n",
      "          2.1767e-01,  1.0841e+00,  2.0507e-01, -2.4842e-01, -5.1159e-02,\n",
      "          2.2705e-01,  2.6323e-01, -5.8206e-01, -2.7321e-01, -4.0025e-02,\n",
      "         -1.8267e-01,  2.5010e-02,  1.6489e-01,  3.5833e-01,  4.4882e-01,\n",
      "          3.3544e-01,  5.0784e-02, -1.9291e-01, -5.4131e-02, -5.4139e-02,\n",
      "          8.4007e-01, -1.1553e-01, -5.4989e-02,  1.7656e-01, -3.1751e-01,\n",
      "          3.1950e-01, -4.2201e-01, -3.5227e-02,  1.8617e-01, -4.3241e-01,\n",
      "          1.5210e-02, -1.2417e-01, -1.1711e-01,  3.1386e-01, -1.1257e-01,\n",
      "          6.7902e-01,  2.7904e-01,  7.5921e-02,  7.1171e-02, -6.1033e-01,\n",
      "         -2.8677e-01,  3.9095e-01, -6.8276e-02,  2.1029e-02,  5.7823e-02,\n",
      "          3.0620e-01,  1.0035e-01, -1.9778e-01, -2.2352e-01,  2.2753e-01,\n",
      "         -4.3440e-03, -2.1313e-01,  2.1982e-04, -2.0353e-01, -6.9408e-01,\n",
      "          3.4912e-01,  4.7537e-02, -2.0347e-01,  5.1482e-01,  1.4935e-01,\n",
      "         -1.5545e-01,  1.1915e-01,  1.8364e-01,  7.5755e-03,  2.2191e-02,\n",
      "          2.7334e-01, -1.2876e-02,  2.5517e-01, -2.8712e-01,  1.8545e-01,\n",
      "         -1.8290e-01, -2.6169e-01, -4.1428e-01, -3.7733e-01,  2.8037e-02,\n",
      "          6.3222e-01, -2.1423e-01, -2.6594e-01,  2.2808e-01,  1.7423e-01,\n",
      "         -7.0074e-02,  2.7293e-01,  4.9335e-02,  1.1503e-01,  3.6859e-02,\n",
      "         -1.5753e-01, -5.9276e-02,  4.3686e-02,  1.5135e-01, -1.2789e-01,\n",
      "         -4.2001e-01, -3.0115e-01,  3.8192e-01,  1.2261e-01,  1.2835e-01,\n",
      "         -1.8300e-01, -2.7547e-01,  9.8484e-02,  2.3829e-01,  2.1989e-01,\n",
      "          1.8655e-01,  3.4404e-01,  1.2262e-01, -1.5742e-01,  8.9674e-03,\n",
      "         -1.5286e-01, -1.6710e-01,  2.5908e-03, -1.5472e-01, -3.6965e-01,\n",
      "         -1.6917e-01,  1.5312e-01, -1.6296e-01,  1.5374e-01, -4.7699e-01,\n",
      "         -1.2731e-01,  1.4247e-01,  3.2402e-02, -1.6121e-01,  2.6641e-01,\n",
      "          3.3335e-01,  4.1422e-01, -9.4435e-02,  4.1489e-01, -2.6854e-01,\n",
      "          2.0782e-01, -1.8809e-01, -4.1493e-03, -9.4899e-02, -3.2422e-01,\n",
      "          1.9351e-02,  3.5326e-01, -2.6149e-01,  3.4512e-01, -1.0067e-02,\n",
      "          2.8385e-01, -3.1002e-01,  5.4783e-03,  2.4396e-01, -5.2707e-01,\n",
      "         -8.1534e-02, -4.7894e-01,  3.5456e-01,  1.2091e-01, -3.1192e-01,\n",
      "          7.6112e-02, -1.1559e-01, -4.7221e+00,  1.9746e-01, -1.3342e-02,\n",
      "          2.3454e-01, -1.6918e-01,  2.3701e-01, -1.0060e-01,  1.9316e-01,\n",
      "          1.4020e-01, -5.2801e-01, -3.5701e-01,  1.5161e-01, -2.1708e-01,\n",
      "          2.8766e-01,  4.7345e-01,  3.5330e-01,  1.2469e-01, -1.0608e-01,\n",
      "         -2.5947e-01, -2.4007e-01, -2.6486e-01, -4.7366e-01,  3.9848e-01,\n",
      "          3.8019e-02, -1.0262e-01, -6.6044e-02, -1.7744e-01, -7.6052e-02,\n",
      "         -3.7446e-01, -7.2946e-03, -7.0517e-01, -3.5486e-01,  2.9142e-01,\n",
      "          4.7436e-02, -3.1053e-01, -2.6321e-01, -5.8188e-02, -1.9379e-01,\n",
      "          3.5902e-01,  4.1386e-01,  1.9855e-02, -9.0717e-02,  1.1982e-01,\n",
      "         -4.2451e-01, -7.7763e-02, -1.2110e-01, -1.2259e-01, -2.8472e-01],\n",
      "        [ 5.3872e-01,  3.8278e-01,  1.3287e-01,  2.5172e-01, -1.2250e-01,\n",
      "          2.0212e-01,  3.7600e-01,  8.6948e-02, -8.8801e-02, -5.1078e-01,\n",
      "         -7.7134e-01,  9.9360e-02, -7.8366e-01,  9.2187e-02, -1.7526e-01,\n",
      "         -1.6007e-01,  5.0210e-01,  9.7711e-02, -5.7988e-02, -3.0246e-01,\n",
      "          1.7500e-01,  4.3428e-01, -1.5867e-01,  4.2111e-01,  5.1620e-01,\n",
      "          1.1901e+00,  5.5721e-01,  5.6702e-01, -2.0313e-01,  6.5468e-01,\n",
      "          2.3057e-01,  1.2476e-01, -3.1220e-01, -8.7629e-01,  1.5300e-01,\n",
      "         -2.6631e-01, -1.9465e-01, -3.0013e-01,  1.9505e-01, -2.8215e-01,\n",
      "          3.1464e-01, -6.9036e-02, -9.2862e-02, -2.6513e-01,  4.6282e-01,\n",
      "         -3.0331e-01,  1.7563e-01,  9.6771e-02, -2.7904e-01, -3.9332e-01,\n",
      "          2.8839e-01, -1.4647e-01,  2.8150e-01, -3.1337e-01, -5.6430e-01,\n",
      "         -2.9045e-01,  5.5505e-02,  2.7922e-01, -3.5095e-01,  3.3064e-02,\n",
      "         -5.3757e-01,  3.5554e-02, -4.1333e-01, -3.8115e-01,  3.8867e-01,\n",
      "          1.3432e-01,  1.2940e-02, -3.0692e-01, -1.7738e-01, -3.2410e-01,\n",
      "         -1.6288e-01,  6.8211e-02,  3.9246e-01, -8.2045e-02,  1.6893e-01,\n",
      "          5.6616e-01, -2.1920e-01,  3.1899e-01, -2.7069e-01,  1.7146e-01,\n",
      "          4.0498e-01,  9.1992e-02, -5.6690e-02,  3.0305e-01, -3.6454e-01,\n",
      "          9.2405e-02, -4.4655e-01, -1.0731e+00, -5.3966e-01, -2.3930e-02,\n",
      "         -1.0646e-01, -1.8318e-01,  1.4313e-01,  3.3855e-01, -7.8222e-01,\n",
      "         -4.4306e-04, -5.2680e-02, -2.5724e-01,  5.0582e-01, -8.3326e-02,\n",
      "         -8.9826e-03, -1.2043e-01,  3.6229e-01,  3.8847e-01,  3.4514e-01,\n",
      "         -4.8466e-01, -7.4994e-02, -1.9785e-01, -2.0485e-01,  1.2593e-01,\n",
      "          1.4742e-01,  3.9396e-01,  4.4383e-01,  1.2739e-01, -6.1103e-01,\n",
      "          5.7033e-01, -2.3778e-01,  2.2256e-01, -1.1186e-01,  3.6443e-01,\n",
      "          1.6126e-01,  2.7152e-01,  1.9605e-01, -3.7924e-01, -1.6088e-01,\n",
      "          8.5627e-02, -1.3461e-01, -5.7987e-01,  5.4019e-01,  2.3625e-01,\n",
      "         -2.2990e-01,  1.5142e-01, -1.0982e-01,  4.3799e-01, -6.2655e-04,\n",
      "          6.5596e-01, -2.4606e-01,  5.5303e-01, -6.7720e-02,  2.6376e-02,\n",
      "          3.3945e-01, -1.5820e-02, -5.5606e-02, -1.1934e-01, -2.2733e-02,\n",
      "         -3.9793e-01, -3.3012e-01,  7.7092e-02, -5.4223e-01,  2.9999e-01,\n",
      "         -3.1376e-01, -3.0162e-01, -6.4881e+00,  2.3074e-01, -2.0690e-02,\n",
      "         -2.5807e-01,  1.8187e-01, -1.5846e-01, -1.9578e-01, -3.3467e-01,\n",
      "         -1.4488e-02,  1.7434e-02, -1.8259e-01, -6.4436e-02,  2.2131e-01,\n",
      "          1.4259e-01,  1.3164e-01, -1.0439e-01,  5.3113e-01, -1.0783e-01,\n",
      "         -8.5021e-03, -4.1719e-01,  4.9453e-01,  2.4615e-01, -3.1403e-01,\n",
      "          1.7314e-01,  5.6110e-01,  3.9689e-02, -1.7334e-01, -1.9232e-01,\n",
      "          3.6296e-01, -4.9952e-01,  2.0313e-01,  6.1971e-02, -4.9404e-01,\n",
      "         -3.3848e-02, -6.5813e-02, -2.1718e-01, -2.1142e-01,  5.7437e-01,\n",
      "         -9.8484e-02, -1.1153e-01,  1.3898e-01,  2.1383e-01, -3.9770e-01,\n",
      "         -2.9110e-01,  5.3160e-01,  3.6646e-01, -1.5511e-01,  2.3963e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(x0.size())\n",
    "print(x1.size())\n",
    "print(Y.size())\n",
    "print(x0[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(OUTPUT_PATH+\"_\"+str(d_model)+\"d.pkl\", \"wb\")\n",
    "pickle.dump(x0, f)\n",
    "pickle.dump(x1, f)\n",
    "pickle.dump(Y, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
